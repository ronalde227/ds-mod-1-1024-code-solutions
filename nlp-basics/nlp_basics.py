# -*- coding: utf-8 -*-
"""nlp-basics.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c7sVT76KdYon_G0xhvyKgF_VSyTpUCP6
"""

import pandas as pd

!pip install nltk

import nltk

nltk.download('gutenberg')

data = pd.read_csv('/content/winemag-data.csv')

data

#1. Tokenize the description feature

from nltk.tokenize import word_tokenize

nltk.download('punkt')
nltk.download('punkt_tab')

data['description']

data['description'][0]

word_tokenize(data['description'][0])

def tokenize(text):
  return word_tokenize(text)

tokenize_list = data['description'].apply(tokenize)

tokenize_list

from nltk.corpus import stopwords

nltk.download('stopwords')

stop_words = stopwords.words('english')

stop_words.extend(['.',',','!','?'])

def remove_stopwords(text):
  return [word for word in text if word.lower() not in stop_words]

stopwords_list = tokenize_list.apply(remove_stopwords)

'this' in stopwords_list[0]

stopwords_list

#3. Stem the tokens
from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer

ps = PorterStemmer()
ls = LancasterStemmer()
ss = SnowballStemmer('english')

def stem_words(text):
  return [ps.stem(word) for word in text]

stemmed_list = stopwords_list.apply(stem_words)

stemmed_list

#4. Lemmatize the tokens
from nltk.stem import WordNetLemmatizer

nltk.download('wordnet')
wnl = WordNetLemmatizer()

nltk.download('averaged_perceptron_tagger')

nltk.download('averaged_perceptron_tagger_eng')

from nltk.corpus import wordnet

def get_wordnet_pos(treebank_tag):

    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN

def lemmatize_words(text):
  #adding tag to each word
  pos_tags = nltk.pos_tag(text)
  return [wnl.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]

  #return [wnl.lemmatize(word, get_wordnet_pos(word)) for word in text]

lemmatized_list = stopwords_list.apply(lemmatize_words)

#5. Create two new features: "Cleaned_Stem_Description" and "Cleaned_Lemma_Description"

def join_words(text):
  return ' '.join(text)

stemmed_list = stemmed_list.apply(join_words)
lemmatized_list = lemmatized_list.apply(join_words)

data["Cleaned_Stem_Description"] = stemmed_list

data['Cleaned_Lemma_Description'] = lemmatized_list

data["Cleaned_Stem_Description"]

data['Cleaned_Lemma_Description']



# Commented out IPython magic to ensure Python compatibility.
#6 Build a word cloud based on Cleaned_Lemma_Description
from wordcloud import WordCloud
import matplotlib.pyplot as plt
# %matplotlib inline

#creating one string from all the strins in data['Cleaned_Lemma_Description']

string = ' '.join(data['Cleaned_Lemma_Description'])

string

word_cloud = WordCloud().generate(string)

plt.imshow(word_cloud)

