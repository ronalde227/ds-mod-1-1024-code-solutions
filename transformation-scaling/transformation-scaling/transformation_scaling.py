# -*- coding: utf-8 -*-
"""transformation-scaling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1705NeWnA9jYwivcDpmm4AU0-eITmBnXF
"""

#Question 1: Which scaling method works best for data with outliers?
#Robust scaling is best for data with outliers since it uses the median and IQR to scale the data. This prevents outliers from being heavily weighted.(Outliers can heavily effect means)

#Question 2: Which scaling method produces data that is normally distributed? What is its mean and variance?
#Standard scaling produces data that is normally distributed. Its mean is 0 and variance is 1.

#Question 3: Which scaling method does not remove sparsity? What is sparsity?
#Max absolute scaling does not remove sparsity. It scales the data to the range [-1, 1] since the the maximum absolute value can never be 0 for any input greater than 0 this means only all 0 values in the the data will return 0.
#Sparse data contain mostly zero values.

#Question 4: Which scaling method is best to use if the bounds of your data are known from domain knowledge?

#Min max scaling is best to use if the bounds of your data are known from domain knowledge because you can use the domain knowledge min and max to scale your data.

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('/content/mtcars_mod.csv')

df

from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, StandardScaler, RobustScaler

#disp minmaxscalar - The values in disp range from 71.1 - 472.0
#since all values are positive, continuous, and we can assume the tru bounds are known(max and mins) we can scale the values to be from 0-1 and preserve the original distribution using minmaxscalar

df['disp'].min()

df['disp'].max()

sns.histplot(df['disp'])

sns.boxplot(df['disp'])

mms = MinMaxScaler()
mms.fit(df[['disp']])

mms.transform(df[['disp']])

df['disp'] = mms.transform(df[['disp']])

#Scaling Hp - Since hp histogram shows the occurance of many 0s the data is sparse. In order to perserve the distribuation of 0s we should use Maxabsolute scalar. However if the 0's are bad data we can us robust scalar but there is only really one outlier.

df['hp'].agg(['min','max'])

df['hp'].value_counts()



sns.histplot(df['hp'])

sns.boxplot(df['hp'])

mas = MaxAbsScaler()

mas.fit(df[['hp']])

df['hp'] = mas.transform(df[['hp']])

df['hp'].agg(['max','min'])

#Scaling drat using robost scaling because boxplot shows 5 outliers in the data

df['drat'].agg(['min','max'])

df['drat'].describe()

sns.histplot(df['drat'])

sns.boxplot(df['drat'])

ros = RobustScaler()

ros.fit(df[['drat']])

df['drat'] = ros.transform(df[['drat']])

df['drat'].agg(['min','max'])

#Using standard scalar to scale wt to achieve normal distribution

sns.histplot(df['wt'])

sns.boxplot(df['wt'])

ss = StandardScaler()

ss.fit(df[['wt']])

df['wt'] = ss.transform(df[['wt']])



#Question 2: Use Box-Cox or Yeo-Johnson to determine the best transformation to use to make mpg and qsec normally distributed. Show the transformed data and the value of $\lambda$.

from scipy.stats import boxcox, yeojohnson
import numpy as np

sns.kdeplot(df['mpg'])

mpg_boxcox , lmbd = boxcox(df['mpg'])

mpg_boxcox

lmbd

sns.kdeplot(mpg_boxcox)

mpg_df = pd.DataFrame(mpg_boxcox)

mpg_df

mpg_df[0].skew()

qsec_yeo , lmbdy = yeojohnson(df['qsec'])

qsec_yeo

lmbdy

qsec_yeo_data = pd.DataFrame(qsec_yeo)

qsec_yeo_data[0].skew()

sns.kdeplot(qsec_yeo)

#3. Normalize all of the numeric rows using L2-normalization and display the new dataframe.

from sklearn.preprocessing import Normalizer

norm2 = Normalizer(norm='l2')

norm2.fit(df.drop('car', axis=1))

df = norm2.transform(df.drop('car', axis=1))

df

